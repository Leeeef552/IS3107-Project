# **Bitcoin Analytics Platform â€” Complete Setup & Deployment Guide**

This guide walks you through the **entire lifecycle** of deploying, initializing, and running the Bitcoin Analytics Platformâ€”from Docker build to Streamlit dashboard interaction.
It integrates environment setup, Airflow DAG sequencing, TimescaleDB validation, real-time streaming, dashboard launch instructions, and project directory structure into a single, cohesive workflow.

---

# **Table of Contents**

1. [Prerequisites](#prerequisites)
2. [Project Directory Structure](#project-directory-structure)
3. [Environment Setup](#environment-setup)
4. [Build Docker Image & Validate API Keys](#build-docker-image--validate-api-keys)
5. [Start the System Using Docker Compose](#start-the-system-using-docker-compose)
6. [Airflow Setup & DAG Execution Order](#airflow-setup--dag-execution-order)
7. [Database Verification](#database-verification)
8. [Start Real-Time Price Stream](#start-real-time-price-stream)
9. [Enable Batch Update Pipelines](#enable-batch-update-pipelines)
10. [Run the Streamlit Dashboard](#run-the-streamlit-dashboard)
11. [Interact With the Dashboard](#interact-with-the-dashboard)
12. [Troubleshooting](#troubleshooting)
13. [Architecture Summary](#architecture-summary)

---

# **Prerequisites**

### **Required Software**

* Python **3.8+**
* Docker Desktop
* Git

Verify installations:

```bash
python --version
docker --version
git --version
```

### **Hardware Requirements**

* **8 GB RAM minimum** (16 GB recommended)
* **10+ GB free disk space**

---

# **Project Directory Structure**

Use this as a reference for where each major component lives.

```

IS3107-Project/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ example.env
â”œâ”€â”€ ml.ipynb
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ batch_update_fng.py
â”‚   â”‚   â”œâ”€â”€ batch_update_price_and_news.py
â”‚   â”‚   â”œâ”€â”€ batch_update_whale.py
â”‚   â”‚   â”œâ”€â”€ fng_init_dag.py
â”‚   â”‚   â”œâ”€â”€ model_prediction_pipeline.py
â”‚   â”‚   â”œâ”€â”€ model_training_pipeline_dag.py
â”‚   â”‚   â”œâ”€â”€ news_init_dag.py
â”‚   â”‚   â”œâ”€â”€ price_init_dag.py
â”‚   â”‚   â””â”€â”€ whale_init_dag.py
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ .streamlit/
â”‚   â”‚   â””â”€â”€ config.toml
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ binance_ws.py
â”‚   â”œâ”€â”€ cryptocompare_orderbook_ws.py
â”‚   â”œâ”€â”€ data_queries.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ historical_data/
â”‚   â””â”€â”€ btcusd_1-min_data.parquet
â”‚
â”œâ”€â”€ prediction/
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â””â”€â”€ predictions_20251115_005321.csv
â”‚   â”œâ”€â”€ prepared/
â”‚   â”‚   â””â”€â”€ inference_data_lb120_20251115_005316.joblib
â”‚   â”œâ”€â”€ preprocessed/
â”‚   â”‚   â”œâ”€â”€ fear_greed_processed.parquet
â”‚   â”‚   â”œâ”€â”€ price_1h_processed.parquet
â”‚   â”‚   â””â”€â”€ sentiment_1h_processed.parquet
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ fear_greed_index.parquet
â”‚       â”œâ”€â”€ historical_price.parquet
â”‚       â””â”€â”€ news_sentiment.parquet
â”‚
â”œâ”€â”€ schema/
â”‚   â”œâ”€â”€ aggregates/
â”‚   â”‚   â”œâ”€â”€ agg_15min.sql
â”‚   â”‚   â”œâ”€â”€ agg_1day.sql
â”‚   â”‚   â”œâ”€â”€ agg_1hour.sql
â”‚   â”‚   â”œâ”€â”€ agg_1month.sql
â”‚   â”‚   â”œâ”€â”€ agg_1w.sql
â”‚   â”‚   â””â”€â”€ agg_5min.sql
â”‚   â”œâ”€â”€ sentiment/
â”‚   â”‚   â”œâ”€â”€ init_sentiment_db.sql
â”‚   â”‚   â””â”€â”€ sentiment_aggregates.sql
â”‚   â”œâ”€â”€ init_price_db.sql
â”‚   â””â”€â”€ init_whale_db.sql
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fng/
â”‚   â”‚   â”œâ”€â”€ init_fng.py
â”‚   â”‚   â””â”€â”€ update_fng.py
â”‚   â”œâ”€â”€ machine_learning_prediction/
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â”œâ”€â”€ prepare_prediction_data.py
â”‚   â”‚   â”œâ”€â”€ preprocess_data.py
â”‚   â”‚   â”œâ”€â”€ pull_data.py
â”‚   â”‚   â””â”€â”€ save_predictions.py
â”‚   â”œâ”€â”€ machine_learning_training/
â”‚   â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ prepare_training_data.py
â”‚   â”‚   â”œâ”€â”€ preprocess_data.py
â”‚   â”‚   â”œâ”€â”€ pull_data.py
â”‚   â”‚   â””â”€â”€ training.py
â”‚   â”œâ”€â”€ news_sentiment/
â”‚   â”‚   â”œâ”€â”€ analyze_sentiment.py
â”‚   â”‚   â”œâ”€â”€ fetch_news.py
â”‚   â”‚   â”œâ”€â”€ load_sentiment.py
â”‚   â”‚   â””â”€â”€ update_sentiment.py
â”‚   â”œâ”€â”€ price/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ backfill_price.py
â”‚   â”‚   â”œâ”€â”€ create_aggregates.py
â”‚   â”‚   â”œâ”€â”€ init_historical_price.py
â”‚   â”‚   â”œâ”€â”€ load_price.py
â”‚   â”‚   â””â”€â”€ update_price.py
â”‚   â”œâ”€â”€ stream_data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ price_stream.py
â”‚   â””â”€â”€ whale/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ extract_large_transactions.py
â”‚       â”œâ”€â”€ fetch_recent_blocks.py
â”‚       â”œâ”€â”€ load_whale_transactions.py
â”‚       â””â”€â”€ transform_whale_sentiments.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluation_results_20251115_004644.joblib
â”‚   â”‚   â””â”€â”€ plots/
â”‚   â”‚       â”œâ”€â”€ error_distribution_20251115_004644.png
â”‚   â”‚       â”œâ”€â”€ forecast_20251115_004644.png
â”‚   â”‚       â””â”€â”€ loss_curves_20251115_004644.png
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ best_model_regression.pth
â”‚   â”‚   â””â”€â”€ training_artifacts_20251115_004638.joblib
â”‚   â”œâ”€â”€ preprocessed/
â”‚   â”‚   â”œâ”€â”€ fear_greed_processed.parquet
â”‚   â”‚   â”œâ”€â”€ price_1h_processed.parquet
â”‚   â”‚   â””â”€â”€ sentiment_1h_processed.parquet
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ fear_greed_index.parquet
â”‚   â”‚   â”œâ”€â”€ historical_price.parquet
â”‚   â”‚   â””â”€â”€ news_sentiment.parquet
â”‚   â””â”€â”€ train_test_splits/
â”‚       â””â”€â”€ training_data_lb60_fh6_20251115_004622.joblib
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ logger.py
```

---

# **Important: Run ALL Commands From the Project Root**

Before running **anything**, ensure you are in:

```bash
cd bitcoin-analytics-platform
```

This ensures:

* Docker builds correctly
* Volume mounts work correctly
* Airflow recognizes DAG files
* Streamlit can import modules
* Python scripts resolve paths correctly

---

# **Environment Setup**

### 1. Copy the sample environment file

```bash
cp example.env .env
```

### 2. Fill all required keys

```env
TIMESCALE_HOST=timescaledb
TIMESCALE_PORT=5432
TIMESCALE_USER=postgres
TIMESCALE_PASSWORD=pass
TIMESCALE_DBNAME=postgres

AIRFLOW_UID=50000
AIRFLOW_GID=0

NEWS_API_KEY=your_key
ALPHA_VANTAGE_API_KEY=your_key
CRYPTOCOMPARE_API_KEY=your_key
REDDIT_CLIENT_ID=your_id
REDDIT_CLIENT_SECRET=your_secret
REDDIT_USER_AGENT=BitcoinSentimentBot/1.0
```

All external APIs must be filled or the DAGs will fail.

---

# **Build Docker Image & Validate API Keys**

### 1. Create required Airflow directories

```bash
mkdir -p airflow/logs airflow/plugins airflow/config
```

### 2. Build the platform image

```bash
docker build -t IS3107-project .
```

### 3. Confirm `.env` contains all variables

```bash
cat .env
```

Double-check API keys.

---

# **Start the System Using Docker Compose**

From the project root:

```bash
docker compose up -d
```

Check all containers:

```bash
docker compose ps
```

Ensure everything is **healthy** before proceeding.

---

# **Airflow Setup & DAG Execution Order**

Access Airflow UI:

```
http://localhost:8080
```

Login:

```
airflow / airflow
```

---

## **1. Initialization DAGs (run FIRST, once each)**

Order inside the group does not matter:

1. `price_init_dag`
2. `whale_init_dag`
3. `fng_init_dag`
4. `news_init_dag` *(~15 min)*

These will:

* Download historical data
* Create TimescaleDB tables & indexes
* Build continuous aggregates

---

## **2. Batch Update DAGs (after init DAGs)**

Run in any order:

* `batch_update_price_and_news`
* `batch_update_whale`
* `batch_update_fng`

These keep your database up to date.

---

## **3. ML DAGs (strict order)**

1. `ml_training_dag`
2. `ml_prediction_dag`

Both must succeed before the dashboard can show forecasts.

---

# **Database Verification**

### Test TimescaleDB connectivity:

```bash
docker exec timescaledb psql -U postgres -d postgres -c "SELECT NOW();"
```

### Validate tables:

```bash
docker exec timescaledb psql -U postgres -d postgres -c "SELECT COUNT(*) FROM historical_price;"
```

```bash
docker exec timescaledb psql -U postgres -d postgres -c "SELECT COUNT(*) FROM news_articles;"
```

```bash
docker exec timescaledb psql -U postgres -d postgres -c "SELECT COUNT(*) FROM whale_transactions;"
```

Expect **non-zero** row counts after initialization DAGs.

---

# **Start Real-Time Price Stream**

Run once DB is initialized:

```bash
python -m scripts.stream_data.price_stream
```

This:

* Connects to Binance WebSocket
* Streams OHLCV candles into TimescaleDB
* Keeps the database updated with real-time prices

Run this **in the background** for continuous updates.

---

# **Enable Batch Update Pipelines**

In Airflow UI, **unpause**:

* `batch_update_price_and_news`
* `batch_update_whale`
* `batch_update_fng`

These maintain fresh data automatically.

---

# **Run the Streamlit Dashboard**

### 1. Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate      # macOS/Linux
.venv\Scripts\activate         # Windows
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Update local `.env` (for dashboard only)

```
TIMESCALE_HOST=localhost
```

### 4. Start Streamlit

```bash
streamlit run dashboard/app.py
```

Access the dashboard:

```
http://localhost:8501
```

---

# **Interact With the Dashboard**

You now have access to:

* ğŸ“ˆ Live OHLCV charts
* ğŸ“Š Market depth & order book
* ğŸ“° News sentiment (FinBERT)
* ğŸ‹ Whale transactions
* ğŸ˜¨ Fear & Greed Index
* ğŸ¤– 12-hour ML forecasts

All components require:

* Initialized database
* ML training + prediction completed
* Live price stream running
* Batch DAGs enabled

---

# **Troubleshooting**

### DAGs not appearing

```bash
ls airflow/dags
```

### Airflow webserver issues

```bash
docker compose restart airflow-webserver
```

### Scheduler issues

```bash
docker compose restart airflow-scheduler
```

### TimescaleDB logs

```bash
docker compose logs timescaledb
```

### Port conflicts

```bash
lsof -i :8080
lsof -i :8501
```

---

# **Architecture Summary**

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚     External Data Sources    â”‚
                 â”‚ Binance | NewsAPI | FNG |    â”‚
                 â”‚ CryptoCompare | Reddit       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                        AIRFLOW PIPELINE
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Init DAGs â†’ Batch DAGs â†’ ML Training â†’ Pred  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
                     TIMESCALEDB
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Historical tables | Live stream | ML tables â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
               STREAMLIT DASHBOARD
```


